{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['寒假', '在家', '又', '捡起', '好久', '不', '写', '的', '博客', '。', '这几天', '断断续续', '的', '在', '改', '自己', '写', '的', '第一', '篇', '论文', '，', '做', '的', '是', '有关', '图像', '处理', '方面', '的', '东西', '。', '打算', '在', '这', '篇', '文章', '完了', '之后', '，', '这', '方面', '就', '告一段落', '了', '吧', '，', '对', '这', '方面', '不是', '很', '能够', '提起', '兴趣', '。', '打算', '之后', '去', '做', '自然', '语言', '处理', '吧', '，', '这不', '是', '最近', '很多', '大牛', '都', '开始', '往', '这个', '方向', '转型', '嘛', '，', '就', '当', '是', '随大流', '了', '。', '今年', '也', '是', '捡', '起来', '原来', '看', '的', '几', '篇', '文章', '，', '把', '深度', '学习', '在', '自然', '语言', '处理', '方面', '的', '方法', '，', '从', '基础', '开始', '再', '回顾', '一', '遍', '。', '今天', '看', '的', '这', '篇', '文章', '呢', '，', '也', '算是', '拉开', '了', '深度', '学习', '在', 'NLP', '方面', '的', '新', '序幕', '吧', '，', '虽然', '说', '新', '的', '方法', '较', '统计', '语言', '模型', '并没有', '在', '效果', '上', '有质', '的', '提升', '，', '但', '对于', '语意', '方面', '的', '处理', '较之', '原来', '的', '方法', '还是', '具有', '较', '大', '的', '优势', '。']\n"
     ]
    }
   ],
   "source": [
    "import pkuseg \n",
    "seg = pkuseg.pkuseg()\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    print(seg.cut(\"寒假在家又捡起好久不写的博客。这几天断断续续的在改自己写的第一篇论文，做的是有关图像处理方面的东西。打算在这篇文章完了之后，这方面就告一段落了吧，对这方面不是很能够提起兴趣。打算之后去做自然语言处理吧，这不是最近很多大牛都开始往这个方向转型嘛，就当是随大流了。今年也是捡起来原来看的几篇文章，把深度学习在自然语言处理方面的方法，从基础开始再回顾一遍。今天看的这篇文章呢，也算是拉开了深度学习在NLP方面的新序幕吧，虽然说新的方法较统计语言模型并没有在效果上有质的提升，但对于语意方面的处理较之原来的方法还是具有较大的优势。\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chinese(uchar):\n",
    "    if uchar >= u'\\u4e00' and uchar <= u'\\u9fa5':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def format_str(content):\n",
    "    content_str = ''\n",
    "    for i in content:\n",
    "        if is_chinese(i):\n",
    "            content_str = content_str + ｉ\n",
    "    return content_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"a.txt\",\"b.txt\",\"c.txt\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    corpus = []\n",
    "    for name in filenames:\n",
    "        with open(name,'r') as f:\n",
    "            str = f.read()\n",
    "            str = format_str(str)\n",
    "            str = seg.cut(str)\n",
    "            corpus.append(\" \".join(str))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer()\n",
    "#该类会将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i 类文本下的词频 \n",
    "transformer=TfidfTransformer()\n",
    "#该类会统计每个词语的tfidf权值 \n",
    "tfidf=transformer.fit_transform(vectorizer.fit_transform( corpus))\n",
    "#第一个fit_transform是计算tf-idf，第二个fit_transform是将 文本转为词频矩阵\n",
    "word=vectorizer.get_feature_names()\n",
    "#获取词袋模型中的所有词语\n",
    "weight=tfidf.toarray()\n",
    "#将tf-idf矩阵抽取出来，元素a[i][j]表示j词在i类文本中的tf-idf 权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.txt : \n",
      "1 人才 0.445315122573092\n",
      "2 海外 0.2385516512687549\n",
      "3 留学 0.21686513751704992\n",
      "b.txt : \n",
      "1 饮食 0.34458017905116933\n",
      "2 习惯 0.24664668976818727\n",
      "3 学生 0.23123127165767557\n",
      "c.txt : \n",
      "1 调查 0.5754079568926591\n",
      "2 调研 0.23643509836573193\n",
      "3 中国 0.17688034529336297\n"
     ]
    }
   ],
   "source": [
    "for (name, w) in zip(filenames,weight):\n",
    "    print(name,\": \")\n",
    "    loc = np.argsort(-w)\n",
    "    for i in range(3):\n",
    "        print(i+1,word[loc[i]], w[loc[i]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
